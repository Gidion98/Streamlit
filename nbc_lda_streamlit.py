# -*- coding: utf-8 -*-
"""NBC - LDA - Streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nE_XG18ev_wsMGa7JNqwiqiZwOZE7qBc

Pertama Instal package yang diperlukan
"""

! pip install --user scipy wordcloud nltk seaborn textblob

"""Import Packagenya dan mount pada google drive untuk mengambil dataset yang diperlukan"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import json, nltk
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
# nltk.download('wordnet')   # for Lemmatization

# %matplotlib inline

"""Pastikan dataset tidak berbentuk UTF-8 encoding"""

total_data = pd.read_csv("/content/drive/My Drive/Copy/Copy of DataSet1.csv", encoding="ISO-8859-1")

"""**Import Contraction**"""

with open('/content/drive/My Drive/Copy/Copy of contractions.json', 'r') as f:
    contractions_dict = json.load(f)
contractions = contractions_dict['contractions']

"""Seting DataFrame Pandas untuk menapilkan tabel yang tidak terpotong"""

pd.set_option('display.max_colwidth', None)

"""Melihat DataSet"""

total_data.head(10)

"""Mengambil nama kolom menjadi variable"""

review = total_data.columns.values[1]
sentiment = total_data.columns.values[2]
review, sentiment

total_data.info()

"""1) Pre-Processing

-> Mengklasifikasikan emotikon
"""

def emoji(review):
    # Senyum -- :), : ), :-), (:, ( :, (-:, :') , :O
    review = re.sub(r'(:\s?\)|:-\)|\(\s?:|\(-:|:\'\)|:O)', ' positiveemoji ', review)
    # Tertawa -- :D, : D, :-D, xD, x-D, XD, X-D
    review = re.sub(r'(:\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', review)
    # Cinta -- <3, :*
    review = re.sub(r'(<3|:\*)', ' positiveemoji ', review)
    # Berkedip -- ;-), ;), ;-D, ;D, (;,  (-; , @-)
    review = re.sub(r'(;-?\)|;-?D|\(-?;|@-\))', ' positiveemoji ', review)
    # Sedih -- :-(, : (, :(, ):, )-:, :-/ , :-|
    review = re.sub(r'(:\s?\(|:-\(|\)\s?:|\)-:|:-/|:-\|)', ' negetiveemoji ', review)
    # Menangis -- :,(, :'(, :"(
    review = re.sub(r'(:,\(|:\'\(|:"\()', ' negetiveemoji ', review)
    return review

"""-> Menentukan fungsi yang akan melakukan PreProcess pada review (Lower Case)"""

import re

def process_review(review):
    if isinstance(review, str):
        review = review.lower()
        review = re.sub('@[^\s]+', '', review)
        review = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', review)
        review = re.sub(r"\d+", " ", review)
        review = re.sub('&quot;', " ", review)
        review = emoji(review)
        review = re.sub(r"\b[a-zA-Z]\b", "", review)
        for word in review.split():
            if word.lower() in contractions:
                review = review.replace(word, contractions[word.lower()])
        review = re.sub(r"[^\w\s]", " ", review)
        review = re.sub(r'(.)\1+', r'\1\1', review)
        review = re.sub(r"\s+", " ", review)
    else:
        review = np.nan  # or any other desired value for missing reviews
    return review

"""-> Buat kolom baru untuk membandingkan review lama dengan review baru"""

total_data['processed_review'] = np.vectorize(process_review)(total_data[review])

"""**Membandingkan data awal dengan data yang sudah dirubah**"""

total_data.head(10)

"""Stemming"""

from nltk.stem.porter import *
stemmer = PorterStemmer()

# total_data = total_data.apply(lambda x: [stemmer.stem(i) for i in x])
# total_data.head(10)

total_data = total_data.apply(lambda x: [stemmer.stem(i) if isinstance(i, str) else i for i in x])
total_data.head(10)

"""Stopword removing"""

stop_words = {"i", "me", "my", "myself", "we", "our", "ours", "ourselves",
            "you", "your", "yours", "yourself", "yourselves", "he", "him",
            "his", "himself", "she", "her", "hers", "herself", "it", "its",
            "itself", "they", "them", "their", "theirs", "themselves", "what",
            "which", "who", "whom", "this", "that", "these", "those", "am", "is",
            "are", "was", "were", "be", "been", "being", "have", "has", "had",
            "having", "do", "does", "did", "doing", "a", "an", "the", "and",
            "but", "if", "or", "because", "as", "until", "while", "of", "at",
            "by", "for", "with", "about", "against", "between", "into", "through",
            "during", "before", "after", "above", "below", "to", "from", "up",
            "down", "in", "out", "on", "off", "over", "under", "again", "further",
            "then", "once", "here", "there", "when", "where", "why", "how", "all",
            "any", "both", "each", "few", "more", "most", "other", "some", "such",
            "only", "own", "same", "so", "than", "too", "very",
            "can", "will", "just", "should", "now", "hate"}
print(stop_words)

"""Bagi menjadi 3 kelas"""

from textblob import TextBlob

def sentiment_analysis(review):
    analysis = TextBlob(review)
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'

total_data['sentiment'] = total_data['processed_review'].apply(sentiment_analysis)
total_data.head(10)

"""Tokenisasi"""

tokenized_review = total_data['processed_review'].apply(lambda x: x.split())
tokenized_review.head(10)

"""Pie Chart"""

sentiment_counts = total_data['sentiment'].value_counts()
sentiments = sentiment_counts.index.tolist()
slices = sentiment_counts.values.tolist()
colors = ['g', 'r', 'b']

fig, ax = plt.subplots()
ax.pie(slices, labels=sentiments, colors=colors, startangle=90, shadow=True,
       explode=(0, 0.1, 0), radius=1.5, autopct='%1.2f%%')
ax.legend()

plt.show()

"""-> Kata Positif"""

positive_words =' '.join([text for text in total_data['processed_review'][total_data[sentiment] !=1]])
wordcloud = WordCloud(width=800, height=500, random_state=21,
            max_font_size=110,background_color="rgba(255, 255, 255, 0)"
            , mode="RGBA").generate(positive_words)
plt.figure(dpi=600)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Used Positive Words")
plt.savefig('/content/drive/My Drive/Copy/Copy of positive_words.png')
plt.show()

"""-> Kata Negatif"""

negetive_words =' '.join([text for text in total_data['processed_review'][total_data[sentiment] == 1]])
wordcloud = WordCloud(width=800, height=500, random_state=21,
            max_font_size=110,background_color="rgba(255, 255, 255, 0)"
            , mode="RGBA").generate(negetive_words)
plt.figure(dpi=600)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title("Most Used Negetive Words")
plt.savefig('/content/drive/My Drive/Copy/Copy of negetive_words.png')
plt.show()

"""Tf-Idf"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf_vectorizer = TfidfVectorizer(use_idf=True,ngram_range=(1,3))
final_vectorized_data = tf_idf_vectorizer.fit_transform(total_data['processed_review'])

feature_names = tf_idf_vectorizer.get_feature_names_out()

for doc_idx, doc in enumerate(final_vectorized_data):
    print("Document ", doc_idx + 1)
    count = 0
    for feature_idx, tfidf_score in zip(doc.indices, doc.data):
        print(feature_names[feature_idx], ": ", tfidf_score)
        count += 1
        if count >= 30:
            break
    if count >= 30:
        break

"""Bagi data untuk mengecek tingkat keakurasiannya"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, total_data['sentiment'],
                                                    test_size=0.2, random_state=69)

"""Cetak dataset yang sudah dibagi"""

print("X_train_shape : ",X_train.shape)
print("X_test_shape : ",X_test.shape)
print("y_train_shape : ",y_train.shape)
print("y_test_shape : ",y_test.shape)

"""Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier

model_naive = MultinomialNB().fit(X_train, y_train)
predicted_naive = model_naive.predict(X_test)

"""Cetak Confusion Matrix"""

from sklearn.metrics import confusion_matrix

plt.figure(dpi=600)
mat = confusion_matrix(y_test, predicted_naive)
sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)

plt.title('Confusion Matrix for Naive Bayes')
plt.xlabel('true label')
plt.ylabel('predicted label')
plt.savefig("/content/drive/My Drive/Copy/Copy of confusion_matrix.png")
plt.show()

"""Cek tingkat ke Akurasian, Precision, Recall, and F-Measure"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

score_naive = accuracy_score(predicted_naive, y_test)
precision_naive = precision_score(y_test, predicted_naive, average='weighted')
recall_naive = recall_score(y_test, predicted_naive, average='weighted')
f1_score_naive = f1_score(y_test, predicted_naive, average='weighted')

print("Accuracy with Naive-bayes: ", score_naive)
print("Precision with Naive-bayes: ", precision_naive)
print("Recall with Naive-bayes: ", recall_naive)
print("F-measure with Naive-bayes: ", f1_score_naive)

"""Classification Report"""

from sklearn.metrics import classification_report
print(classification_report(y_test, predicted_naive))

"""Dari sini sudah LDA"""

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

word2vec_model = Word2Vec(tokenized_review, vector_size=200, window=5, min_count=2, sg=1, workers=4)
word2vec_model.train(tokenized_review, total_examples=len(tokenized_review), epochs=10)

word2vec_model.wv.most_similar('good')

word2vec_model.wv.most_similar('bad')

total_data.head(10) #menampilkan dataframe

document = []

for i in range(len(tokenized_review)):
    if len(tokenized_review.iloc[i]) >= 4:
        a=tokenized_review.iloc[i][3]
        document.append(a)

document[0:50]

doc_clean = tokenized_review
doc_clean[0:10]

"""Proses topic modeling dengan LDA gensim"""

import gensim
from gensim import corpora

dictionary = corpora.Dictionary(doc_clean)
print(dictionary)

doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]

# Membuat object untuk LDA model menggunakan gensim library
Lda = gensim.models.ldamodel.LdaModel

total_topics = 3 # jumlah topik yang akan di extract
number_words = 10 # jumlah kata per topik

# Jalankan dan Uji LDA model pada document term matrix.
lda_model = Lda(doc_term_matrix, num_topics=total_topics, id2word = dictionary, passes=50)

lda_model.show_topics(num_topics=total_topics, num_words=number_words)

# Word Count of Topic Keywords

from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in doc_clean for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df_imp_wcount = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])
print(df_imp_wcount)

from google.colab import drive
drive.mount('drive')

#simpan ke google drive
df_imp_wcount.to_csv('df_imp_wcount.csv')
!cp df_imp_wcount.csv "/content/drive/My Drive/Copy/"

#Dominant topic and its percentage contribution in each topic
def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=document):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

print(df_dominant_topic.head(10))

#simpan ke google drive
df_dominant_topic.to_csv('df_dominant_topic.csv')
!cp df_dominant_topic.csv "/content/drive/My Drive/Copy/"

!pip install pyLDAvis==3.2.1
!pip install pandas==1.5.3

import pyLDAvis.gensim
import pickle
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()

import os
LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(total_topics))

corpus = [dictionary.doc2bow(text) for text in doc_clean]

# proses ini mungkin agak lama
import pyLDAvis.gensim


if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus,  dictionary)
    with open(LDAvis_data_filepath, 'wb') as f:
        pickle.dump(LDAvis_prepared, f)

# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)

pyLDAvis.save_html(LDAvis_prepared, '/content/drive/My Drive/Copy/ldavis_prepared_'+ str(total_topics) +'.html')

# proses ini mungkin agak lama
LDAvis_prepared

"""Run streamlit menggunakan ngrok, agar lebih mudah karena banyak tutorial di YOUTUBE

> https://www.youtube.com/watch?v=MUD-pBOnvdo&ab_channel=1littlecoder

> https://www.youtube.com/watch?v=x0NdZkaciws&ab_channel=JCharisTech

NB : Streamlitnya tidak bisa dijalankan difile terpisah
"""

!pip install streamlit -q

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import matplotlib.pyplot as plt
# import pandas as pd
# import nltk
# import numpy as np
# from nltk.stem import PorterStemmer
# nltk.download('punkt')
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# 
# # Menambahkan judul aplikasi
# st.write('# Sentiment Analysis')
# 
# 
# # Menampilkan data dalam bentuk tabel
# st.write('### DataSet' )
# total_data = pd.read_csv('/content/drive/My Drive/Copy/Copy of DataSet1.csv', encoding='ISO-8859-1')
# data = total_data.head(10)
# st.dataframe(data)
# 
# 
# # Merubah data menjadi lower case
# st.write('### Lower Case' )
# import re
# review = total_data.columns.values[1]
# sentiment = total_data.columns.values[2]
# def process_review(review):
#     if isinstance(review, str):
#         review = review.lower()
#         review = re.sub('@[^\s]+', '', review)
#         review = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', review)
#         review = re.sub(r"\d+", " ", review)
#         review = re.sub('&quot;', " ", review)
#         review = re.sub(r"\b[a-zA-Z]\b", "", review)
#         review = re.sub(r"[^\w\s]", " ", review)
#         review = re.sub(r'(.)\1+', r'\1\1', review)
#         review = re.sub(r"\s+", " ", review)
#     else:
#         review = np.nan  # or any other desired value for missing reviews
#     return review
# total_data['processed_review'] = np.vectorize(process_review)(total_data[review])
# st.dataframe(total_data.head(10))
# 
# 
# # Menampilkan data stemmer
# st.write('### Stemming' )
# stemmer = PorterStemmer()
# total_data = total_data.apply(lambda x: [stemmer.stem(i) if isinstance(i, str) else i for i in x])
# st.dataframe(total_data.head(10))
# 
# 
# # Menampilkan data stopword removing
# st.write('### Stopword Removing' )
# stopwords_english = set(stopwords.words('english'))
# def remove_stopwords(text):
#     tokens = word_tokenize(text)
#     filtered_tokens = [word for word in tokens if word.lower() not in stopwords_english]
#     return ' '.join(filtered_tokens)
# total_data['processed_review'] = total_data['processed_review'].apply(remove_stopwords)
# st.dataframe(total_data.head(10))
# 
# 
# # Membagi data menjadi 3 kelas
# st.write('### Membagi Data Menjadi 3 Kelas' )
# from textblob import TextBlob
# review = total_data.columns.values[1] #content
# sentiment = total_data.columns.values[2] #score
# def sentiment_analysis(review):
#     analysis = TextBlob(review)
#     if analysis.sentiment.polarity > 0:
#         return 'positive'
#     elif analysis.sentiment.polarity == 0:
#         return 'neutral'
#     else:
#         return 'negative'
# total_data['sentiment'] = ''
# total_data['sentiment'] = total_data['processed_review'].apply(sentiment_analysis)
# st.dataframe(total_data.head(10))
# 
# 
# # Menampilkan data tokenized
# st.write('### Tokenized' )
# tokenized_review = total_data['processed_review'].apply(lambda x: x.split())
# st.dataframe(tokenized_review.head(10))
# 
# 
# # Menampilkan pie chart untuk persentase sentimen
# st.write('### Sentiment Distribution' )
# sentiment_counts = total_data['sentiment'].value_counts()
# sentiments = sentiment_counts.index.tolist()
# slices = sentiment_counts.values.tolist()
# colors = ['g', 'r', 'b']
# 
# fig, ax = plt.subplots()
# ax.pie(slices, labels=sentiments, colors=colors, startangle=90, shadow=True,
#        explode=(0, 0.1, 0), autopct='%1.2f%%')
# ax.legend()
# st.pyplot(fig)
# 
# 
# # Menampilkan word cloud untuk kata-kata positif
# st.write('### Word Cloud - Positive Words' )
# st.image('/content/drive/My Drive/Copy/Copy of positive_words.png')
# 
# 
# # Menampilkan word cloud untuk kata-kata negatif
# st.write('### Word Cloud - Negative Words' )
# st.image('/content/drive/My Drive/Copy/Copy of negetive_words.png')
# 
# 
# # Menampilkan Tf-Idf
# st.write('### Menampilkan Tf-Idf' )
# from sklearn.feature_extraction.text import TfidfVectorizer
# tf_idf_vectorizer = TfidfVectorizer(use_idf=True,ngram_range=(1,3))
# final_vectorized_data = tf_idf_vectorizer.fit_transform(total_data['processed_review'])
# feature_names = tf_idf_vectorizer.get_feature_names_out()
# for doc_idx, doc in enumerate(final_vectorized_data):
#     st.write("Document ", doc_idx + 1)
#     count = 0
#     for feature_idx, tfidf_score in zip(doc.indices, doc.data):
#         st.write(feature_names[feature_idx], ": ", tfidf_score)
#         count += 1
#         if count >= 30:
#             break
#     if count >= 30:
#         break
# 
# 
# # Membagi data menjadi data train dan data test
# # st.write('### Bagi Data Menjadi Data Train dan Data Test')
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, total_data['sentiment'],
#                                                     test_size=0.2, random_state=69)
# 
# # st.write("X-Train : ",X_train.shape)
# # st.write("X-Test : ",X_test.shape)
# # st.write("Y-Train : ",y_train.shape)
# # st.write("Y-Test : ",y_test.shape)
# 
# 
# # Jalankan Program Naive Bayes
# from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier
# 
# model_naive = MultinomialNB().fit(X_train, y_train)
# predicted_naive = model_naive.predict(X_test)
# 
# 
# # Menampilkan confusion matrix
# st.write('### Confusion Matrix' )
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import confusion_matrix
# 
# plt.figure(dpi=600)
# mat = confusion_matrix(y_test, predicted_naive)
# sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)
# 
# plt.title('Confusion Matrix')
# plt.xlabel('True Label')
# plt.ylabel('Predicted Label')
# 
# fig = plt.gcf()
# fig.set_size_inches(10, 8)
# st.pyplot(fig)
# 
# 
# # Menampilkan Akurasi, Precision, Recall, F-Measure
# st.write('### Cek Tingkat Akurasi, Precision, Recall, F-Measure')
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# 
# score_naive = accuracy_score(predicted_naive, y_test)
# precision_naive = precision_score(y_test, predicted_naive, average='weighted')
# recall_naive = recall_score(y_test, predicted_naive, average='weighted')
# f1_score_naive = f1_score(y_test, predicted_naive, average='weighted')
# 
# st.write("Accuracy : ", score_naive)
# st.write("Precision : ", precision_naive)
# st.write("Recall : ", recall_naive)
# st.write("F-measure : ", f1_score_naive)
# 
# 
# # Menampilkan Classification Report
# from sklearn.metrics import classification_report
# st.write('### Classification Report')
# st.text(classification_report(y_test, predicted_naive))
# # Versi Table
# #classification_report_str = classification_report(y_test, predicted_naive)
# #classification_report_dict = classification_report(y_test, predicted_naive, output_dict=True)
# #classification_report_df = pd.DataFrame(classification_report_dict).transpose()
# #st.dataframe(classification_report_df)
# 
# 
# # Representasi vector kata / mencari kata yang mirip
# st.write('### Mencari Kata Yang Mirip')
# from gensim.models import Word2Vec
# from gensim.models import KeyedVectors
# word2vec_model = Word2Vec(tokenized_review, vector_size=200, window=5, min_count=2, sg=1, workers=4)
# word2vec_model.train(tokenized_review, total_examples=len(tokenized_review), epochs=10)
# st.write('Kata Yang Mirip Dengan "Good" :')
# st.dataframe(word2vec_model.wv.most_similar('good'))
# st.write('Kata Yang Mirip Dengan "Bad" :')
# st.dataframe(word2vec_model.wv.most_similar('bad'))
# 
# 
# # Preprocessing Topik Modeling
# st.write('### Preprocessing Topik Modeling ')
# st.text('Document')
# document = []
# 
# for i in range(len(tokenized_review)):
#     if len(tokenized_review.iloc[i]) >= 4:
#         a=tokenized_review.iloc[i][3]
#         document.append(a)
# st.dataframe(document[0:10])
# 
# st.text('Doc_Clean')
# doc_clean = tokenized_review
# st.dataframe(doc_clean[0:10])
# 
# 
# # Proses Topik Modeling
# import gensim
# from gensim import corpora
# 
# dictionary = corpora.Dictionary(doc_clean)
# st.write('### Proses Topik Modeling')
# st.text('Dictionary')
# st.write(dictionary)
# 
# doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
# 
# # Membuat object untuk LDA model menggunakan gensim library
# Lda = gensim.models.ldamodel.LdaModel
# 
# total_topics = 3 # jumlah topik yang akan di extract
# number_words = 10 # jumlah kata per topik
# 
# # Jalankan dan Uji LDA model pada document term matrix.
# lda_model = Lda(doc_term_matrix, num_topics=total_topics, id2word = dictionary, passes=50)
# st.text('Pengujian LDA Model Pada Document Term Matrix')
# st.dataframe(lda_model.show_topics(num_topics=total_topics, num_words=number_words))
# 
# # Word Count of Topic Keywords
# from collections import Counter
# topics = lda_model.show_topics(formatted=False)
# data_flat = [w for w_list in doc_clean for w in w_list]
# counter = Counter(data_flat)
# 
# out = []
# for i, topic in topics:
#     for word, weight in topic:
#         out.append([word, i , weight, counter[word]])
# 
# df_imp_wcount = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])
# st.text('Topik Word Count')
# st.dataframe(df_imp_wcount)
# 
# #Dominant topic and its percentage contribution in each topic
# def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=document):
#     # Init output
#     sent_topics_df = pd.DataFrame()
# 
#     # Get main topic in each document
#     for i, row_list in enumerate(ldamodel[corpus]):
#         row = row_list[0] if ldamodel.per_word_topics else row_list
#         # print(row)
#         row = sorted(row, key=lambda x: (x[1]), reverse=True)
#         # Get the Dominant topic, Perc Contribution and Keywords for each document
#         for j, (topic_num, prop_topic) in enumerate(row):
#             if j == 0:  # => dominant topic
#                 wp = ldamodel.show_topic(topic_num)
#                 topic_keywords = ", ".join([word for word, prop in wp])
#                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
#             else:
#                 break
#     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
# 
#     # Add original text to the end of the output
#     contents = pd.Series(texts)
#     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
#     return(sent_topics_df)
# 
# df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)
# 
# # Format
# df_dominant_topic = df_topic_sents_keywords.reset_index()
# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
# 
# st.text('Dominant Topik')
# st.dataframe(df_dominant_topic.head(10))
# 
# import pickle
# import pyLDAvis
# import pyLDAvis.gensim
# # Visualize the topics
# #pyLDAvis.enable_notebook()
# 
# import os
# LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(total_topics))
# 
# corpus = [dictionary.doc2bow(text) for text in doc_clean]
# 
# # proses ini mungkin agak lama
# import pyLDAvis.gensim
# 
# # Memeriksa apakah LDAvis data sudah ada
# if not os.path.exists(LDAvis_data_filepath):
#     # Proses ini mungkin memakan waktu lama
#     LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
#     with open(LDAvis_data_filepath, 'wb') as f:
#         pickle.dump(LDAvis_prepared, f)
# else:
#     # Memuat data LDAvis yang sudah ada
#     with open(LDAvis_data_filepath, 'rb') as f:
#         LDAvis_prepared = pickle.load(f)
# 
# # Menyimpan visualisasi dalam format HTML
# html_filepath = '/content/drive/My Drive/Copy/ldavis_prepared_' + str(total_topics) + '.html'
# pyLDAvis.save_html(LDAvis_prepared, html_filepath)
# 
# # Menampilkan visualisasi di Streamlit
# st.write('### Visualisasi LDA')
# st.components.v1.html(open(html_filepath, 'r', encoding='utf-8').read(), width=900, height=800)

#!pip uninstall pyngrok  #Digunakan saat ngrok.kill() tidak bisa digunakan
!pip install pyngrok

from pyngrok import ngrok

ngrok.set_auth_token("2SJmZNYWEFONDKZWu9Ohng2QFLw_3d7WWwX49RrPDqFSjip3a")

"""Untuk mengulang ngrok"""

#ngrok.kill()

!nohup streamlit run app.py --server.port 8050 &
ngrok_tunnel = ngrok.connect(8050)
url = ngrok_tunnel
print(url)